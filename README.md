# Interpretable NLP
This is a list of NLP interpretability research, which mainly covers recent papers on ACL, EMNLP, NAACL, ICLR, NIPS, ICML, etc.

***Welcome to contribute!*** 

Please follow the template and send the PR: **++[Paper Title](https://www.google.com) (Venue Year)++**

#### Classification
- [EDUCE: Explaining model Decisions through Unsupervised Concepts Extraction](https://arxiv.org/abs/1905.11852) (Arxiv 2019)
- [Is Attention Interpretable?](https://arxiv.org/pdf/1906.03731) (NAACL 2019)
- [Attention is not Explanation](https://arxiv.org/abs/1902.10186) (NAACL 2019)
- [Towards Explainable NLP: A Generative Explanation Framework for Text Classification](https://arxiv.org/abs/1811.00196) (ACL 2019)
- [Interpretable Neural Predictions with Differentiable Binary Variables](https://arxiv.org/pdf/1905.08160.pdf) (ACL 2019)
- [How Important Is a Neuron?](https://arxiv.org/abs/1805.12233) (ICLR 2019)
- [Understanding Convolutional Neural Networks for Text Classification](https://arxiv.org/abs/1809.08037) (EMNLP 2018 Workshop)
- [Beyond Word Importance Contextual Decomposition to Extract Interactions from LSTMs](https://arxiv.org/abs/1801.05453) (ICLR 2018)
- [Automatic Rule Extraction From LSTM Networks](https://arxiv.org/abs/1702.02540) (ICLR 2017)
- [Understanding Neural Networks through Representation Erasure](https://arxiv.org/abs/1612.08220) (Arxiv 2016) 
- [Explaining Predictions of Non-Linear Classifiers in NLP](https://www.aclweb.org/anthology/W16-1601) (ACL 2016 Workshop)
- [Rationalizing Neural Predictions](https://people.csail.mit.edu/taolei/papers/emnlp16_rationale.pdf) (EMNLP 2016)
- [Comparing Automatic and Human Evaluation of Local Explanations for Text Classification](https://www.aclweb.org/anthology/N18-1097) (NAACL 2018)

#### Sequence to Sequence
- [Identifying and Controlling Important Neurons in Neural Machine Translation](https://arxiv.org/abs/1811.01157) (ICLR 2019)
- [What Is One Grain of Sand in the Desert? Analyzing Individual Neurons in Deep NLP Models](https://arxiv.org/abs/1812.09355) (AAAI 2019)
- [SEQ2SEQ-VIS: A Visual Debugging Tool for Sequence-to-Sequence Models](https://arxiv.org/abs/1804.09299) (IEEE VIS 2018)
- [Did the Model Understand the Question?](https://www.aclweb.org/anthology/P18-1176) (ACL 2018)
- [Pathologies of Neural Models Make Interpretations Difficult](https://aclweb.org/anthology/D18-1407) (EMNLP 2018)
- [Latent Alignment and Variational Attention](http://papers.nips.cc/paper/8179-latent-alignment-and-variational-attention.pdf) (NIPS 2018)
- [LSTMVis: A Tool for Visual Analysis of Hidden State Dynamics in Recurrent Neural Networks](https://arxiv.org/abs/1606.07461) (IEEE TVCG 2018)
- [Visualizing and Understanding Neural Machine Translation](https://www.aclweb.org/anthology/P17-1106) (ACL 2017)
- [A Causal Framework for Explaining the Predictions of Black-box Sequence-to-Sequence Models](https://arxiv.org/abs/1707.01943) (EMNLP 2017)
- [Axiomatic Attribution for Deep Networks](https://arxiv.org/abs/1703.01365) (ICML 2017)
- [Visualizing and Understanding Neural Models in NLP](https://www.aclweb.org/anthology/N16-1082) (NAACL 2016)

#### Sequence Labeling
- [Explaining Character-Aware Neural Networks for Word-Level Prediction: Do They Discover Linguistic Rules?](https://www.aclweb.org/anthology/D18-1365) (EMNLP 2018)

#### Others (e.g., NLI, Embedding)
- [Learning Corresponded Rationales for Text Matching](https://openreview.net/forum?id=rklQas09tm) (ICLR 2019 Submission)
- [Interpretable Neural Architectures for Attributing an Adâ€™s Performance to its Writing Style](https://aclweb.org/anthology/papers/W/W18/W18-5415/) (EMNLP 2018 Workshop)
- [Interpreting Recurrent and Attention-Based Neural Models: a Case Study on Natural Language Inference](https://arxiv.org/pdf/1808.03894.pdf) (EMNLP 2018)
- [SPINE: SParse Interpretable Neural Embeddings](https://arxiv.org/abs/1711.08792) (AAAI 2018)